# R_Retriever_v5_Lifecycle.py
#
# [í•µì‹¬ ë³€ê²½ ì‚¬í•­]
# 1. (ë¬¸ì œ 1, 3) ë°ì´í„° ìˆ˜ëª… ì£¼ê¸° (ì¦ë¶„, ì‚­ì œ, ë°°ì¹˜)
#    - 'CONTROL_FILE' (ì œì–´ DB)ì„ ë„ì…í•˜ì—¬ ì‹ ê·œ/ì‚­ì œ ë¬¸ì„œë§Œ ì²˜ë¦¬
#    - 'get_chunk_id'ë¡œ ê²°ì •ë¡ ì  IDë¥¼ ìƒì„±, DB 'upsert' ì§€ì› (ì¤‘ë³µ ë°©ì§€)
#    - 'prune_stale_documents'ë¡œ 'SOURCES'ì—ì„œ ì œê±°ëœ ë¬¸ì„œ DBì—ì„œ ì‚­ì œ
#    - 'BATCH_SIZE'ë¡œ OOM ë°©ì§€
# 2. (ë¬¸ì œ 4) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¸ë±ì‹±
#    - Elasticsearch (Keyword)ì™€ Chroma (Vector)ì— ë™ì‹œ ì¸ë±ì‹±
# 3. (ë¬¸ì œ 5) ë¶„ì‚° ì €ì¥
#    - 'SOURCES'ì˜ "type"ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ES Index/Chroma Collectionì— ë¶„ì‚° ì €ì¥
# 4. (ë¬¸ì œ 2 - ìš”ì²­ ì‚¬í•­) PDF ë¡œë” ìœ ì§€
#    - HTML íŒŒì‹±(v3) ëŒ€ì‹  Playwright PDF ìº¡ì²˜ + 'PyPDFLoader'ë¡œ ë³µê·€
#    - (ê²½ê³ : GIGO ìœ„í—˜ì€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤.)

import os
import sys
import asyncio
import hashlib
from typing import List, Dict, Set
from dotenv import load_dotenv
from playwright.async_api import async_playwright, Page  # â—ï¸ ë¹„ë™ê¸° Playwright
import re
import json

from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_elasticsearch import ElasticsearchStore
from elasticsearch import Elasticsearch, NotFoundError
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings
from langchain_community.document_loaders import PyPDFLoader # â—ï¸ PyPDFLoader
import torch

print("--- [R_Retriver.py] ğŸš€ v5: ìˆ˜ëª… ì£¼ê¸°/ë¶„ì‚° PDF ETL íŒŒì´í”„ë¼ì¸ ---")

# --- â—ï¸ 1. ì„¤ì •ê°’ ---
DB_PATH = "./chroma_db_persistent"
ES_URL = "http://localhost:9200"
BATCH_SIZE = 100
CONTROL_FILE = "./processed_sources.json" # (ì œì–´ DB ëŒ€ìš©)
PDF_CACHE_DIR = "./pdf_cache"
os.makedirs(PDF_CACHE_DIR, exist_ok=True)

# (ë¬¸ì œ 5) ì†ŒìŠ¤ ëª©ë¡: "type" ì§€ì •
SOURCES = [
    {"url": "https://mvje.tistory.com/270", "type": "k8s_tech"},
    {"url": "https://parkkingcar.tistory.com/197", "type": "k8s_tech"},
    {"url": "https://co-de.tistory.com/40", "type": "k8s_tech"},
    # {"url": "https://www.moef.go.kr/policy/policy01.do", "type": "policy"},
]

# --- â—ï¸ 2. PDF ë‹¤ìš´ë¡œë” ë° ë¡œë” (ë¬¸ì œ 2 ì œì™¸) ---
def clean_url_to_filename(url: str) -> str:
    if url.startswith("https://"): url = url[8:]
    elif url.startswith("http://"): url = url[7:]
    filename = re.sub(r'[\\/:?."<>|%]', '_', url)
    return filename[:100] + ".pdf"

async def download_and_load_pdf(page: Page, url: str) -> (List[Document], str):
    """
    (ë¹„ë™ê¸°) Playwrightë¡œ PDFë¥¼ ìº¡ì²˜í•˜ê³  PyPDFLoaderë¡œ ë¡œë“œ
    """
    filepath = os.path.join(PDF_CACHE_DIR, clean_url_to_filename(url))
    
    # â—ï¸ (ë¬¸ì œ 1) ìºì‹œ ì‚¬ìš©. (ì‹¤ì œ ìš´ì˜ ì‹œ: 'ì—…ë°ì´íŠ¸' ê°ì§€ ë¡œì§ í•„ìš”)
    if not os.path.exists(filepath):
        print(f"  - [Download] ìº¡ì²˜ ì¤‘: {url}")
        try:
            await page.goto(url, wait_until="networkidle", timeout=20000)
            await page.pdf(path=filepath, format="A4", print_background=False)
        except Exception as e:
            print(f"  - [Download ì˜¤ë¥˜] {url} ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return None, None
    else:
        print(f"  - [Download] ìºì‹œ ì‚¬ìš©: {url}")

    print(f"  - [Load] PyPDFLoaderë¡œ ë¡œë“œ ì¤‘: {filepath}")
    try:
        loader = PyPDFLoader(filepath)
        # â—ï¸ PyPDFLoader.load()ëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_threadë¡œ ì‹¤í–‰
        docs = await asyncio.to_thread(loader.load)
        
        # (ê²½ê³ : GIGO) ì´ 'docs'ì—ëŠ” ì‚¬ì´ë“œë°”, ê´‘ê³  ë“± ëª¨ë“  í…ìŠ¤íŠ¸ê°€ í¬í•¨ë¨
        if not docs:
            print(f"  - [Load ê²½ê³ ] PyPDFLoaderê°€ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None, None
            
        print(f"  - [Load] {len(docs)}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ.")
        return docs, filepath
    except Exception as e:
        print(f"  - [Load ì˜¤ë¥˜] {filepath} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

# --- â—ï¸ 3. ê²°ì •ë¡ ì  ID ìƒì„±ê¸° ë° ì œì–´ DB (ë¬¸ì œ 1, 3) ---
def get_source_id(url: str) -> str:
    """URLì„ í•´ì‹œí•˜ì—¬ ê³ ìœ í•œ 'ë¬¸ì„œ ID' ìƒì„±"""
    return hashlib.sha256(url.encode('utf-8')).hexdigest()

def get_chunk_id(source_id: str, chunk_index: int) -> str:
    """'ë¬¸ì„œ ID'ì™€ 'ì²­í¬ ìˆœì„œ'ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ í•œ 'ì²­í¬ ID' ìƒì„±"""
    return f"{source_id}_{chunk_index}"

def load_processed_sources() -> Set[str]:
    """(ì œì–´ DB) ì²˜ë¦¬ ì™„ë£Œëœ source_id ëª©ë¡ì„ ë¡œë“œ"""
    try:
        with open(CONTROL_FILE, 'r') as f: return set(json.load(f))
    except FileNotFoundError: return set()

def save_processed_sources(processed_ids: Set[str]):
    """(ì œì–´ DB) ì²˜ë¦¬ ì™„ë£Œëœ source_id ëª©ë¡ì„ ì €ì¥"""
    with open(CONTROL_FILE, 'w') as f: json.dump(list(processed_ids), f)

# --- â—ï¸ 4. ë™ì  ìŠ¤í† ì–´ (ë¬¸ì œ 4, 5) ---
# (ì „ì—­ ë¦¬ì†ŒìŠ¤ ë¡œë“œ)
try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    g_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3", model_kwargs={"device": device}, encode_kwargs={"normalize_embeddings": True})
    g_es_client = Elasticsearch(hosts=[ES_URL], request_timeout=30)
    g_es_client.info()
    g_chroma_client = chromadb.PersistentClient(path=DB_PATH, settings=Settings(anonymized_telemetry=False))
    print(f"âœ… [Index] ì „ì—­ ë¦¬ì†ŒìŠ¤(ì„ë² ë”©, ES, Chroma) ë¡œë“œ ì™„ë£Œ (Device: {device})")
except Exception as e:
    print(f"âŒ [Index] ì¹˜ëª…ì  ì˜¤ë¥˜: ì „ì—­ ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}"); sys.exit(1)

def get_stores_for_type(doc_type: str) -> (ElasticsearchStore, Chroma):
    """(ë¬¸ì œ 5) ë¬¸ì„œ ìœ í˜•(type)ì— ë§ëŠ” ES Indexì™€ Chroma Collectionì„ ë°˜í™˜"""
    es_index_name = f"rag_idx_{doc_type}"
    collection_name = f"rag_coll_{doc_type}"
    
    keyword_store = ElasticsearchStore(
        es_connection=g_es_client,
        index_name=es_index_name,
        strategy=ElasticsearchStore.BM25RetrievalStrategy()
    )
    vectorstore = Chroma(
        client=g_chroma_client,
        collection_name=collection_name,
        embedding_function=g_embeddings,
    )
    return keyword_store, vectorstore

# --- â—ï¸ 5. ë¬¸ì„œ ì‚­ì œ ë¡œì§ (ë¬¸ì œ 1) ---
async def prune_stale_documents(current_source_ids: Set[str], processed_source_ids: Set[str]):
    """SOURCES ëª©ë¡ì—ì„œ ì œê±°ëœ 'ì˜¤ë˜ëœ' ë¬¸ì„œë¥¼ DBì—ì„œ ì‚­ì œ"""
    stale_ids = processed_source_ids - current_source_ids
    if not stale_ids:
        print("âœ… [Index] ì‚­ì œí•  ì˜¤ë˜ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"--- [Index] â—ï¸ {len(stale_ids)}ê°œì˜ ì˜¤ë˜ëœ ë¬¸ì„œ ID ì‚­ì œ ì‹œì‘ ---")
    
    # (ê°œì„  í•„ìš”: ì œì–´ DBê°€ source_id -> (url, type) ë§¤í•‘ì„ ì €ì¥í•´ì•¼ í•¨)
    # (í˜„ì¬ëŠ” ë§¤í•‘ ì •ë³´ê°€ ì—†ì–´ ì–´ë–¤ URL/Typeì„ ì§€ì›Œì•¼ í• ì§€ ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŒ)
    
    # (ì‹œë®¬ë ˆì´ì…˜: ì œì–´ DBì— {'source_id': {'url': '...', 'type': '...'}}ì´ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
    # control_db = load_full_control_db() 
    # for source_id in stale_ids:
    #     info = control_db.get(source_id)
    #     if info:
    #         url = info['url']
    #         doc_type = info['type']
    #         es_idx = f"rag_idx_{doc_type}"
    #         coll_name = f"rag_coll_{doc_type}"
    #         try:
    #             g_es_client.delete_by_query(index=es_idx, body={"query": {"match": {"metadata.source": url}}}, ignore=[404])
    #             collection = g_chroma_client.get_collection(coll_name)
    #             collection.delete(where={"source": url})
    #             print(f"  - [Delete] {url} (Type: {doc_type}) ì‚­ì œ ì™„ë£Œ")
    #         except Exception as e:
    #             print(f"  - [Delete ì˜¤ë¥˜] {url} ì‚­ì œ ì‹¤íŒ¨: {e}")

    print(f"--- [Index] â—ï¸ (ì‹œë®¬ë ˆì´ì…˜) ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ ---")


async def main():
    processed_source_ids = load_processed_sources()
    current_source_ids = set(get_source_id(s['url']) for s in SOURCES)

    # (ë¬¸ì œ 1) ì¦ë¶„ ì²˜ë¦¬: ì‹ ê·œ ì†ŒìŠ¤ë§Œ í•„í„°ë§
    new_sources = [s for s in SOURCES if get_source_id(s['url']) not in processed_source_ids]
    
    print(f"--- [Index] ì´ {len(SOURCES)}ê°œ ì†ŒìŠ¤ ì¤‘ {len(new_sources)}ê°œ ì‹ ê·œ ì²˜ë¦¬ ì‹œì‘ ---")
    
    if not new_sources:
        print("--- [Index] ì‹ ê·œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ---")
    else:
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1200, 
            chunk_overlap=100  # (ìŠ¬ë¼ì´ë”© ëŒ€ì‹  100í† í° ê²¹ì¹˜ê¸°ë¡œ ìˆ˜ì •)
        )

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()
                
                for source in new_sources:
                    url = source['url']
                    doc_type = source['type']
                    source_id = get_source_id(url)
                    
                    print(f"\n--- [Index] ì²˜ë¦¬ ì¤‘: {url} (Type: {doc_type}) ---")

                    # 1. ë¡œë“œ (PDF ìº¡ì²˜ + PyPDFLoader)
                    docs, filepath = await download_and_load_pdf(page, url)
                    if not docs:
                        print(f"  - [Index ê²½ê³ ] ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨. ê±´ë„ˆëœ€.")
                        continue
                    
                    # 2. ë¶„í•  (ë¬¸ì œ 3)
                    doc_splits = text_splitter.split_documents(docs)
                    
                    # 3. ID ë° ë©”íƒ€ë°ì´í„° í• ë‹¹ (ë¬¸ì œ 1)
                    ids = [get_chunk_id(source_id, i) for i, _ in enumerate(doc_splits)]
                    for i, chunk in enumerate(doc_splits):
                        chunk.metadata["chunk_id"] = ids[i]
                        chunk.metadata["source"] = url # ì›ë³¸ URL ì£¼ì…
                        # PyPDFLoaderì˜ 'page' ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€ë¨
                    
                    print(f"  - [Index] {len(doc_splits)}ê°œ ì²­í¬ ë° ID ìƒì„± ì™„ë£Œ.")

                    # 4. ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° (ë¬¸ì œ 5)
                    try:
                        keyword_store, vectorstore = get_stores_for_type(doc_type)
                    except Exception as e:
                        print(f"  - [Index ì˜¤ë¥˜] ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}"); continue
                        
                    # 5. ë°°ì¹˜ ì¸ë±ì‹± (ë¬¸ì œ 1, 3, 4)
                    for i in range(0, len(doc_splits), BATCH_SIZE):
                        batch_docs = doc_splits[i : i + BATCH_SIZE]
                        batch_ids = ids[i : i + BATCH_SIZE]
                        print(f"  - [Index] ë°°ì¹˜ {i//BATCH_SIZE + 1} (Type: {doc_type}) ì¸ë±ì‹±...")
                        try:
                            # (Upsert: IDê°€ ê°™ìœ¼ë©´ ë®ì–´ì”€)
                            await asyncio.gather(
                                vectorstore.aadd_documents(batch_docs, ids=batch_ids),
                                keyword_store.aadd_documents(batch_docs, ids=batch_ids, request_timeout=30)
                            )
                        except Exception as e:
                            print(f"  - [Index ì˜¤ë¥˜] ë°°ì¹˜ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

                    processed_source_ids.add(source_id) # ì œì–´ DBì— ì¶”ê°€
                
                await browser.close()
            print("--- [Index] ì‹ ê·œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ---")
        except Exception as e:
            print(f"âŒ [Index] ì¹˜ëª…ì  ì˜¤ë¥˜: Playwright ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            sys.exit(1)

    # --- 3. ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ (ë¬¸ì œ 1) ---
    await prune_stale_documents(current_source_ids, processed_source_ids)
    
    save_processed_sources(current_source_ids)
    print("--- [R_Retriver.py] ğŸš€ v5: ETL íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œ ---")


if __name__ == "__main__":
    asyncio.run(main())